{
 "cells": [
  {
   "cell_type": "code",
   "id": "ce6757ffa4dee466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:52:18.574270Z",
     "start_time": "2025-12-24T04:52:16.262789Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import os"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dam/miniforge3/envs/poemas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "7d8881d4c3cae0ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:52:21.860810Z",
     "start_time": "2025-12-24T04:52:21.858403Z"
    }
   },
   "source": [
    "torch.mps.empty_cache()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "dtype = torch.float16\n",
    "BASE_MODEL = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "PATH_ANAFORA = \"lora_anafora\"\n",
    "PATH_METAFORA = \"modelo_lora_final\"\n",
    "PATH_ALITERACION = \"lora_aliteracion\"\n",
    "PATH_PARALELISMO = \"lora_paralelismo\"\n",
    "PATH_POLISINDENTON = \"lora_polisindeton\"\n",
    "PATH_ASINDENTON = \"lora_asindeton\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "ab6ef6fddc45686a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:52:31.983481Z",
     "start_time": "2025-12-24T04:52:23.500241Z"
    }
   },
   "source": [
    "print(\"Tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "print(\"Modelo base directamente en MPS\")\n",
    "# Cargamos directamente en el dispositivo para evitar duplicidad RAM -> MPS\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    dtype=dtype,\n",
    "    device_map={\"\": \"mps\"}, # Forzamos el mapa al dispositivo MPS directamente\n",
    "    trust_remote_code=True,\n",
    "\n",
    "    # Para ver la atención\n",
    "    attn_implementation=\"eager\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer\n",
      "Modelo base directamente en MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "a74357be43921453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:52:54.850434Z",
     "start_time": "2025-12-24T04:52:49.733809Z"
    }
   },
   "source": [
    "print(\"LoRA\")\n",
    "try:\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        PATH_METAFORA,\n",
    "        adapter_name=\"metafora\"\n",
    "    )\n",
    "    model.load_adapter(PATH_ALITERACION, adapter_name=\"aliteracion\")\n",
    "    model.load_adapter(PATH_ANAFORA, adapter_name=\"anafora\")\n",
    "    model.load_adapter(PATH_PARALELISMO, adapter_name=\"paralelismo\")\n",
    "    model.load_adapter(PATH_POLISINDENTON, adapter_name=\"polisindeton\")\n",
    "    model.load_adapter(PATH_ASINDENTON, adapter_name=\"asindenton\")\n",
    "\n",
    "    model.eval()\n",
    "    print(f\"ADAPTADORES: {list(model.peft_config.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar adaptadores: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA\n",
      "ADAPTADORES: ['metafora', 'aliteracion', 'anafora', 'paralelismo', 'polisindeton', 'asindenton']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "811783d55f8e0a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:52:57.893512Z",
     "start_time": "2025-12-24T04:52:57.888865Z"
    }
   },
   "source": [
    "def generar_poema(palabra, figura=\"metafora\"):\n",
    "    model.set_adapter(figura)\n",
    "\n",
    "    prompt = f'Escribe un poema usando la figura retórica \"{figura}\" con la palabra \"{palabra}\".\\nPoema:\\n'\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,      # Reducimos un poco para mayor velocidad\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True          # Crucial para velocidad en inferencia\n",
    "        )\n",
    "\n",
    "    resultado = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    poema = resultado.split(\"Poema:\")[-1].strip()\n",
    "\n",
    "    print(f\"\\n[{figura.upper()}] {palabra}:\\n\" + \"\" + f\"\\n{poema}\\n\" + \"-\"*30)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T02:52:53.644266Z",
     "start_time": "2025-12-24T02:52:53.638455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualizar_atencion(palabra, figura=\"metafora\", capa=-1, cabeza=0):\n",
    "    # Activación del adaptador dinámico sobre el modelo base [cite: 56, 86]\n",
    "    model.set_adapter(figura)\n",
    "\n",
    "    prompt = f'Escribe un poema usando la figura retórica \"{figura}\" con la palabra \"{palabra}\".\\nPoema:\\n'\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Ejecución indicando explícitamente la salida de las matrices de atención\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        # attentions es una tupla: (capas, batch, cabezas, seq_len, seq_len)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "    # Extracción de la matriz para una capa y cabeza específica\n",
    "    # Por defecto usamos la última capa (-1) y la primera cabeza (0)\n",
    "    attn_matrix = attentions[capa][0, cabeza].cpu().float().numpy()\n",
    "\n",
    "    # Conversión de IDs a tokens legibles para los ejes del gráfico\n",
    "    tokens = [tokenizer.decode([t]) for t in inputs['input_ids'][0]]\n",
    "\n",
    "    # Generación del gráfico\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, cmap='magma', annot=False)\n",
    "    plt.title(f\"Atención Dinámica: {figura.upper()} - Capa {capa}\")\n",
    "    plt.xlabel(\"Tokens de Entrada\")\n",
    "    plt.ylabel(\"Tokens de Salida\")\n",
    "    plt.show()"
   ],
   "id": "dd29967986f127f8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T02:57:23.268108Z",
     "start_time": "2025-12-24T02:57:23.250704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualizar_atencion_completa(palabra, figura=\"anafora\", capa=-1, cabeza=0):\n",
    "    # El Selector Dinámico activa los pesos de LoRA correspondientes [cite: 81]\n",
    "    model.set_adapter(figura)\n",
    "\n",
    "    prompt = f'Escribe un poema usando la figura retórica \"{figura}\" con la palabra \"{palabra}\".\\nPoema:\\n'\n",
    "    inputs_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Es crucial que ambos pasos estén dentro de no_grad para gestionar la memoria en tu Mac\n",
    "    with torch.no_grad():\n",
    "        # Generación del poema basado en el entrenamiento previo [cite: 45]\n",
    "        output_tokens = model.generate(\n",
    "            **inputs_prompt,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        # Forward pass para obtener las matrices de atención del texto completo\n",
    "        outputs = model(output_tokens, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "    # .detach() elimina el rastro de gradientes para permitir la conversión a NumPy\n",
    "    attn_matrix = attentions[capa][0, cabeza].detach().cpu().float().numpy()\n",
    "    full_tokens = [tokenizer.decode([t]) for t in output_tokens[0]]\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(attn_matrix, xticklabels=full_tokens, yticklabels=full_tokens, cmap='magma')\n",
    "    plt.title(f\"Atención Completa: {figura.upper()} (Prompt + Generación)\")\n",
    "    plt.show()"
   ],
   "id": "9f0bc2c6c5be2f6b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "d714dd26a021769b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:53:29.979964Z",
     "start_time": "2025-12-24T04:53:04.663777Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    generar_poema(\"Nube\", \"aliteracion\")\n",
    "    generar_poema(\"Nube\", \"metafora\")\n",
    "    generar_poema(\"Nube\", \"anafora\")\n",
    "    generar_poema(\"Nube\", \"paralelismo\")\n",
    "    generar_poema(\"Nube\", \"polisindeton\")\n",
    "    generar_poema(\"Nube\", \"asindenton\")\n",
    "\n",
    "\n",
    "\n",
    "    #visualizar_atencion(\"Vida\", \"anafora\")\n",
    "\n",
    "    #visualizar_atencion_completa(\"Vida\", \"anafora\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ALITERACION] Nube:\n",
      "\n",
      "En raudales rocosos, rugen las nubes,\n",
      "gruñendo gritos de granizo y garra.\n",
      "Zozobran zarpas de viento, zarandeando,\n",
      "truenos trastornan tempestades torpes.\n",
      "<|endoftext|>\n",
      "------------------------------\n",
      "\n",
      "[METAFORA] Nube:\n",
      "\n",
      "Nube susurra en el alba,\n",
      "Silencio teje entre sueños y realidad.\n",
      "Cae sobre mentes, olas de pensamiento,\n",
      "En la vastedad del cielo se funden.\n",
      "<|endoftext|>\n",
      "------------------------------\n",
      "\n",
      "[ANAFORA] Nube:\n",
      "\n",
      "En el cielo nubes danzan con gracia ligera\n",
      "En el cielo nubes se esconden tras las ramas altas  \n",
      "En el cielo nubes reflejan el sol en tonos pálidos\n",
      "En el cielo nubes llevan sueños a los vientos que fluyen\n",
      "<|endoftext|>\n",
      "------------------------------\n",
      "\n",
      "[PARALELISMO] Nube:\n",
      "\n",
      "Si en el cielo fluye la Nube,\n",
      "entonces en mi alma se asoma,\n",
      "si en la noche su sombra se derrama,\n",
      "entonces en mis sueños se ala.\n",
      "<|endoftext|>\n",
      "------------------------------\n",
      "\n",
      "[POLISINDETON] Nube:\n",
      "\n",
      "Y la nube y el cielo y las estrellas y la luna,\n",
      "y susurra y flota y desaparece y regresa.\n",
      "Y los pájaros y el viento y la brisa y el sol,\n",
      "y se extiende y se agita y se disipa y permanece.\n",
      "<|endoftext|>\n",
      "------------------------------\n",
      "\n",
      "[ASINDENTON] Nube:\n",
      "\n",
      "La nube, el cielo, las estrellas,\n",
      "canta, llora, suspira, vuela,\n",
      "el sol, la luna, los rayos,\n",
      "brilla, oculta, desaparece.\n",
      "<|endoftext|>\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
